{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ou6S4TnNafhH",
      "metadata": {
        "id": "Ou6S4TnNafhH"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marketing-and-customer-insight/AI_For_Marketing/blob/main/AI%20for%20Image%20Classification/Run_VisionTransformer.ipynb)\n",
        "\n",
        "In Google Colab please make sure to select: Runtime -> Change Runtime -> Tesla T4 (GPU) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6f5b29",
      "metadata": {
        "id": "4a6f5b29"
      },
      "source": [
        "# Vision Transformer Image Classification\n",
        "\n",
        "This notebook demonstrates how to train and use a ConvNeXt vision transformer model for image classification using the Hugging Face transformers library. The workflow includes:\n",
        "- Loading and preparing image datasets\n",
        "- Hyperparameter tuning with K-fold cross-validation\n",
        "- Training the model on your custom dataset\n",
        "- Making predictions on new, unlabeled images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cfa117c",
      "metadata": {
        "id": "0cfa117c"
      },
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "The following cell imports all necessary libraries for:\n",
        "- Data manipulation (pandas, numpy)\n",
        "- Deep learning (torch, transformers)\n",
        "- Image processing (PIL, datasets)\n",
        "- Model evaluation and metrics\n",
        "- Cross-validation and data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yUUNdfXNWWnS",
      "metadata": {
        "id": "yUUNdfXNWWnS"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c28e87-6ab8-4001-b422-2598d0d74ed4",
      "metadata": {
        "id": "70c28e87-6ab8-4001-b422-2598d0d74ed4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "from PIL import Image\n",
        "from datasets import Dataset, Features, Value, Image\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import (\n",
        "    AutoFeatureExtractor,\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForImageClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from evaluate import load\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a0cc59",
      "metadata": {
        "id": "33a0cc59"
      },
      "source": [
        "Download the image datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33557bc9",
      "metadata": {
        "id": "33557bc9"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 --filter=blob:none --sparse \\\n",
        "https://github.com/marketing-and-customer-insight/AI_For_Marketing.git\n",
        "%cd AI_For_Marketing\n",
        "!git sparse-checkout set \"AI for Image Classification/Datasets\"\n",
        "!mv \"AI for Image Classification/Datasets\" /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1409eca0",
      "metadata": {
        "id": "1409eca0"
      },
      "source": [
        "## 2. Configuration Settings\n",
        "\n",
        "Configure the core settings for training:\n",
        "- **MODEL_HUGGINGFACE**: The pre-trained model to use (ConvNeXt from Hugging Face Model Hub)\n",
        "- **DATASET_PATH**: Path to your dataset CSV file (must contain 'image_path' and 'label' columns)\n",
        "- **PERFORMANCE_SUMMARY_PATH**: Where to save the training results summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0eb984",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c0eb984",
        "outputId": "21af652f-d40c-4876-c0dc-074724c1a06c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Settings\n",
        "\n",
        "Where should the trained model be saved, where is your dataset located?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Check if running in Google Colab to set appropriate paths\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    BASE_PATH = '/content'\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    BASE_PATH = '.'\n",
        "\n",
        "MODEL_HUGGINGFACE = 'facebook/convnext-base-384-22k-1k'\n",
        "MODEL_NAME = 'ConvNeXt'\n",
        "\n",
        "PERFORMANCE_SUMMARY_PATH = 'training_summary_vision_transformer.csv'\n",
        "DATASET_PATH = f'{BASE_PATH}/Datasets/Brand_Selfies/dataset.csv'\n",
        "MODEL_OUT_DIR = f'{BASE_PATH}/model_output'\n",
        "\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "print(f\"Model output directory: {MODEL_OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dac5a41",
      "metadata": {
        "id": "7dac5a41"
      },
      "source": [
        "## 3. Training Hyperparameters\n",
        "\n",
        "Set the hyperparameters to test during model training. The training process will test all combinations of:\n",
        "- **EPOCHS**: Number of training epochs (32 max)\n",
        "- **BATCH_SIZES**: Batch sizes for training\n",
        "- **LEARNING_RATES**: Learning rates for the optimizer\n",
        "\n",
        "The best hyperparameters will be identified through 10-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41b9a21",
      "metadata": {
        "id": "a41b9a21"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Settings\n",
        "\n",
        "You can modify these settings for your training process\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 32\n",
        "BATCH_SIZES = [16]\n",
        "LEARNING_RATES = [1e-5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33356ffc",
      "metadata": {
        "id": "33356ffc"
      },
      "source": [
        "## 4. Helper Functions\n",
        "\n",
        "This cell defines two key functions:\n",
        "\n",
        "**`create_classification_dataset()`**:\n",
        "- Converts your CSV dataset into a Hugging Face dataset format\n",
        "- Maps class labels to numeric IDs\n",
        "- Loads images and applies the image processor\n",
        "\n",
        "**`train_hf_classification_model()`**:\n",
        "- Configures and trains the model using Hugging Face Trainer\n",
        "- Computes performance metrics (precision, recall, F1, accuracy)\n",
        "- Implements early stopping to prevent overfitting\n",
        "- Returns trained model and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503ca4eb-e933-4352-969d-8620863822b1",
      "metadata": {
        "id": "503ca4eb-e933-4352-969d-8620863822b1"
      },
      "outputs": [],
      "source": [
        "def create_classification_dataset(DF_DATASET, MODELNAME):\n",
        "    df = DF_DATASET.copy()\n",
        "    unique_labels = sorted(df[\"label\"].astype(str).unique().tolist())\n",
        "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    df[\"labels\"] = df[\"label\"].astype(str).map(label2id)\n",
        "    img_path_list = df.image_path.to_list()\n",
        "    label_id_list = df.labels.to_list()\n",
        "    assert len(img_path_list) == len(label_id_list)\n",
        "\n",
        "    features = datasets.Features({\n",
        "        \"image_path\": datasets.Value(\"string\"),\n",
        "        \"img\": datasets.Image(),\n",
        "        \"labels\": datasets.ClassLabel(names=unique_labels),\n",
        "    })\n",
        "\n",
        "    ds = datasets.Dataset.from_dict(\n",
        "        {\n",
        "            \"img\": img_path_list,\n",
        "            \"image_path\": img_path_list,\n",
        "            \"labels\": label_id_list,\n",
        "        },\n",
        "        features=features,\n",
        "    )\n",
        "    processor = AutoImageProcessor.from_pretrained(MODELNAME)\n",
        "\n",
        "    def hf_transform(example_batch):\n",
        "        inputs = processor(\n",
        "            [x.convert(\"RGB\") for x in example_batch[\"img\"]],\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs[\"labels\"] = example_batch[\"labels\"]\n",
        "        inputs[\"image_path\"] = example_batch[\"image_path\"]\n",
        "        return inputs\n",
        "\n",
        "    prepared_ds = ds.with_transform(hf_transform)\n",
        "    return prepared_ds, label2id, processor\n",
        "\n",
        "def train_hf_classification_model(outdir, epochs, batch_size, learning_rate, train_dataset, test_dataset, MODEL_NAME):\n",
        "        def collate_fn(batch):\n",
        "            return {\n",
        "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "            'labels': torch.tensor([x['labels'] for x in batch])\n",
        "            }\n",
        "        def custom_metrics(eval_pred):\n",
        "            metric1 = load(\"precision\")\n",
        "            metric2 = load(\"recall\")\n",
        "            metric3 = load(\"f1\")\n",
        "            metric4 = load(\"accuracy\")\n",
        "\n",
        "            logits, labels = eval_pred\n",
        "            predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "            precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
        "            recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
        "            f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
        "            accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "            return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}\n",
        "\n",
        "        labels = train_dataset.features['labels'].names\n",
        "        processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForImageClassification.from_pretrained(MODEL_NAME, num_labels=len(labels), ignore_mismatched_sizes=True, id2label={str(i): c for i, c in enumerate(labels)}, label2id={c: str(i) for i, c in enumerate(labels)})\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "\n",
        "        early_stopping_patience_epochs = 3\n",
        "        early_stopping = EarlyStoppingCallback(early_stopping_patience=early_stopping_patience_epochs)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "        output_dir = outdir,\n",
        "        disable_tqdm=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        num_train_epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        remove_unused_columns=False,\n",
        "        push_to_hub=False,\n",
        "        load_best_model_at_end=True,\n",
        "        fp16=True\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            data_collator=collate_fn,\n",
        "            compute_metrics=custom_metrics,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=test_dataset,\n",
        "            callbacks=[early_stopping]\n",
        "        )\n",
        "\n",
        "        train_results = trainer.train()\n",
        "        log_history = trainer.state.log_history\n",
        "        best_epoch = None\n",
        "        eval_entries = [e for e in log_history if \"eval_loss\" in e or \"eval_accuracy\" in e]\n",
        "        if eval_entries:\n",
        "            if any(\"eval_loss\" in e for e in eval_entries):\n",
        "                best_entry = min(\n",
        "                    (e for e in eval_entries if \"eval_loss\" in e),\n",
        "                    key=lambda x: x[\"eval_loss\"],\n",
        "                )\n",
        "            else:\n",
        "                best_entry = max(\n",
        "                    eval_entries,\n",
        "                    key=lambda x: x.get(\"eval_accuracy\", float(\"-inf\")),\n",
        "                )\n",
        "            best_epoch = best_entry.get(\"epoch\")\n",
        "        if best_epoch is None:\n",
        "            best_epoch = trainer.state.epoch\n",
        "        optimal_epochs_trained = int(round(best_epoch)) if best_epoch is not None else None\n",
        "        metrics = trainer.evaluate(test_dataset)\n",
        "        return trainer, model, metrics, optimal_epochs_trained"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7502602c",
      "metadata": {
        "id": "7502602c"
      },
      "source": [
        "## 5. Load Your Dataset\n",
        "\n",
        "Load the CSV file containing your image paths and labels. Your dataset CSV must contain:\n",
        "- **image_path**: Path to each image file\n",
        "- **label**: The class label for that image\n",
        "\n",
        "The dataset is then displayed to verify it loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WXp7cWP6X1Tv",
      "metadata": {
        "id": "WXp7cWP6X1Tv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e8b67e-06dd-49c2-a520-7597c475e074",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "00e8b67e-06dd-49c2-a520-7597c475e074",
        "outputId": "fbf06a44-4873-494d-eee8-8fc223b269a5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Loading your dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "required_cols = {'image_path', 'label'}\n",
        "assert required_cols.issubset(df.columns), \\\n",
        "    'Please make sure that your dataset contains both columns: image_path and label.'\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a7b913",
      "metadata": {
        "id": "07a7b913"
      },
      "source": [
        "## 6. Hyperparameter Tuning and Model Training\n",
        "\n",
        "This is the main training cell that:\n",
        "\n",
        "1. **Grid Search**: Tests all combinations of batch sizes and learning rates\n",
        "2. **K-Fold Cross-Validation**: Trains 10 separate models with different train/validation splits to ensure robust results\n",
        "3. **Tracks Performance**: Saves accuracy, precision, recall, and F1 scores for each fold\n",
        "4. **Finds Best Hyperparameters**: Identifies the combination that yields the highest validation accuracy\n",
        "5. **Final Training**: Trains a final model using the best hyperparameters on 90% of the data\n",
        "6. **Saves Model**: Stores the trained model and processor for later inference\n",
        "\n",
        "The results are saved to a CSV file for easy review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9108c9-ad7d-4894-bba5-5e5d3f3410fa",
      "metadata": {
        "id": "2a9108c9-ad7d-4894-bba5-5e5d3f3410fa"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Fine-tune the model with your dataset\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "df_train_summary = pd.DataFrame()\n",
        "print('Training models to find optimal hyperparameters.')\n",
        "for LEARNING_RATE in LEARNING_RATES:\n",
        "    for BATCH_SIZE in BATCH_SIZES:\n",
        "        dataset, label2id, processor = create_classification_dataset(df, MODELNAME=MODEL_HUGGINGFACE)\n",
        "        ds = dataset.shuffle(seed=1)\n",
        "\n",
        "        fold_counter = 0\n",
        "        kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        for train_index, val_index in kf.split(ds):\n",
        "            fold_counter += 1\n",
        "            train_dataset = ds.select(train_index)\n",
        "            val_dataset = ds.select(val_index)\n",
        "\n",
        "            trainer, model, metrics, optimal_epochs_trained = train_hf_classification_model(outdir=MODEL_OUT_DIR, epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE, train_dataset=train_dataset, test_dataset=val_dataset, MODEL_NAME=MODEL_HUGGINGFACE)\n",
        "\n",
        "            i = df_train_summary.shape[0]\n",
        "            df_train_summary.at[i, 'Batch_Size'] = BATCH_SIZE\n",
        "            df_train_summary.at[i, 'Learning_Rate'] = LEARNING_RATE\n",
        "            df_train_summary.at[i, 'Fold'] = fold_counter\n",
        "            df_train_summary.at[i, 'Epochs'] = optimal_epochs_trained\n",
        "            df_train_summary.at[i, 'Accuracy'] = np.round(metrics['eval_accuracy'], 4)\n",
        "            df_train_summary.at[i, 'Precision'] = np.round(metrics['eval_precision'], 4)\n",
        "            df_train_summary.at[i, 'Recall'] = np.round(metrics['eval_recall'], 4)\n",
        "            df_train_summary.at[i, 'F1'] = np.round(metrics['eval_f1'], 4)\n",
        "            df_train_summary.to_csv(PERFORMANCE_SUMMARY_PATH, index=False)\n",
        "\n",
        "print('Training the best model for inference.')\n",
        "df_avg = (\n",
        "    df_train_summary\n",
        "    .groupby([\"Batch_Size\", \"Learning_Rate\"], as_index=False)\n",
        "    .agg(\n",
        "        AVG_ACCURACY=(\"Accuracy\", \"mean\"),\n",
        "        STD_ACCURACY=(\"Accuracy\", \"std\"),\n",
        "        N_FOLDS=(\"Accuracy\", \"count\"),\n",
        "        AVG_OPTIMAL_EPOCHS=(\"Epochs\", \"mean\"),\n",
        "    )\n",
        ")\n",
        "df_avg = df_avg.sort_values(\"AVG_ACCURACY\", ascending=False)\n",
        "\n",
        "best_row = df_avg.iloc[0]\n",
        "best_batch_size = int(best_row[\"Batch_Size\"])\n",
        "best_learning_rate = float(best_row[\"Learning_Rate\"])\n",
        "\n",
        "split = dataset.train_test_split(test_size=0.1, seed=1)\n",
        "train_dataset = split[\"train\"]\n",
        "val_dataset = split[\"test\"]\n",
        "\n",
        "trainer, model, metrics, optimal_epochs_trained = train_hf_classification_model(outdir=MODEL_OUT_DIR, epochs=EPOCHS, batch_size=best_batch_size, learning_rate=best_learning_rate, train_dataset=train_dataset, test_dataset=val_dataset, MODEL_NAME=MODEL_HUGGINGFACE)\n",
        "model.config.label2id = label2id\n",
        "model.config.id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "FINAL_MODEL_DIR = os.path.join(MODEL_OUT_DIR, \"trained_tvm\")\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "processor.save_pretrained(FINAL_MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb487ac",
      "metadata": {
        "id": "5eb487ac"
      },
      "source": [
        "## 7. Inference with Trained Model\n",
        "\n",
        "The following cells show how to load your trained model and make predictions on new, unlabeled images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb75d4b",
      "metadata": {
        "id": "2cb75d4b"
      },
      "source": [
        "## 8. Load the Trained Model\n",
        "\n",
        "Load the saved model and processor that were trained in the previous section. The model is moved to the available device (GPU if available, CPU otherwise) and set to evaluation mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae32bb39-7a87-4a30-a8a2-4f1c605b7a0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "575f6559bb94476eb031aeef30640aa2",
            "18b37e6dcfb34f09935f1db4bf1dd34d",
            "0b4b21647f7c41fcae4e6f9c906ce3be",
            "f6a27ce560de4fb7bf10731a95d1fb56",
            "c46c68dbdd9641968e7a50c14c918760",
            "ae53526a054145129e53602ff3e3d2c3",
            "4368991e97cf45419b2d81a2c4b83eb9",
            "a14f9033fce140de90cee99376d90c53",
            "daa5e1ba5d7047f28e70a5e51df5307a",
            "72a86e60ed2046708ebe8a3f09dd8c07",
            "41a9ae92ee6848228f5a385f6b1a07da"
          ]
        },
        "id": "ae32bb39-7a87-4a30-a8a2-4f1c605b7a0b",
        "outputId": "ff63ec89-f280-4756-81ea-fe6d11248f14"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Use the trained model for prediction of unlabeled images\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "FINAL_MODEL_DIR = os.path.join(MODEL_OUT_DIR, \"trained_tvm\")\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(FINAL_MODEL_DIR)\n",
        "processor = AutoImageProcessor.from_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4c7a65",
      "metadata": {
        "id": "df4c7a65"
      },
      "source": [
        "## 9. Prepare Images for Prediction\n",
        "\n",
        "Load all images from the directory containing unseen/unlabeled images that you want to classify. A DataFrame is created to store the image paths and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea29cdf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea29cdf4",
        "outputId": "bbc29fe2-00b1-4190-8556-3d4259c9d84f"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "# Use the BASE_PATH from the configuration settings\n",
        "prediction_image_paths = glob.glob(f'{BASE_PATH}/Datasets/Brand_Selfies/Unseen_Samples/*')\n",
        "df_predictions = pd.DataFrame(prediction_image_paths, columns=['image_path'])\n",
        "\n",
        "print(f\"Found {len(prediction_image_paths)} images for prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e850544e",
      "metadata": {
        "id": "e850544e"
      },
      "source": [
        "## 10. Run Inference on Unlabeled Images\n",
        "\n",
        "This cell processes each image through the trained model to generate predictions:\n",
        "1. Opens each image and converts it to RGB format\n",
        "2. Processes the image using the trained processor\n",
        "3. Runs the model in inference mode (no gradient calculation)\n",
        "4. Extracts the predicted class label\n",
        "5. Saves all predictions to a CSV file for review\n",
        "\n",
        "The output CSV contains the image paths and their corresponding predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636a73f9-18b9-4a15-8703-9992e5af3c2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "636a73f9-18b9-4a15-8703-9992e5af3c2f",
        "outputId": "64866281-0692-46f7-e6a6-a48a53f031a1"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(df_predictions.index):\n",
        "    image = Image.open(df_predictions.at[i, 'image_path']).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    pred_ids = logits.argmax(dim=-1).tolist()\n",
        "    pred_labels = [model.config.id2label[i] for i in pred_ids]\n",
        "    df_predictions.at[i, 'prediction'] = pred_labels[0]\n",
        "\n",
        "output_path = f'{BASE_PATH}/prediction_unseen_data_vision_transformer.csv'\n",
        "df_predictions.to_csv(output_path, index=False)\n",
        "print(f\"\\n✓ Predictions saved to: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b4b21647f7c41fcae4e6f9c906ce3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14f9033fce140de90cee99376d90c53",
            "max": 344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daa5e1ba5d7047f28e70a5e51df5307a",
            "value": 344
          }
        },
        "18b37e6dcfb34f09935f1db4bf1dd34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae53526a054145129e53602ff3e3d2c3",
            "placeholder": "​",
            "style": "IPY_MODEL_4368991e97cf45419b2d81a2c4b83eb9",
            "value": "Loading weights: 100%"
          }
        },
        "41a9ae92ee6848228f5a385f6b1a07da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4368991e97cf45419b2d81a2c4b83eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575f6559bb94476eb031aeef30640aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18b37e6dcfb34f09935f1db4bf1dd34d",
              "IPY_MODEL_0b4b21647f7c41fcae4e6f9c906ce3be",
              "IPY_MODEL_f6a27ce560de4fb7bf10731a95d1fb56"
            ],
            "layout": "IPY_MODEL_c46c68dbdd9641968e7a50c14c918760"
          }
        },
        "72a86e60ed2046708ebe8a3f09dd8c07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14f9033fce140de90cee99376d90c53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae53526a054145129e53602ff3e3d2c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46c68dbdd9641968e7a50c14c918760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa5e1ba5d7047f28e70a5e51df5307a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6a27ce560de4fb7bf10731a95d1fb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72a86e60ed2046708ebe8a3f09dd8c07",
            "placeholder": "​",
            "style": "IPY_MODEL_41a9ae92ee6848228f5a385f6b1a07da",
            "value": " 344/344 [00:00&lt;00:00, 698.57it/s, Materializing param=convnext.layernorm.weight]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
