{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a1108d",
   "metadata": {},
   "source": [
    "# BLIP Image Captioning for Influencer Content Analysis\n",
    "\n",
    "This notebook demonstrates how to use BLIP (Bootstrapping Language-Image Pre-training), a state-of-the-art vision language model, to automatically generate captions for images in a local folder.\n",
    "\n",
    "## What is BLIP Captioning?\n",
    "\n",
    "BLIP is a multimodal AI model that can understand images and generate human-readable text descriptions. You can use it in two ways:\n",
    "\n",
    "1. **Unconditional Captioning**: The model generates a complete caption from scratch based on what it sees in the image\n",
    "2. **Conditional Captioning**: You can provide a prompt (e.g., \"a photo of\"), and the model completes the caption based on your direction\n",
    "\n",
    "## Use Case for Influencer Marketing\n",
    "\n",
    "Automatically generating captions for influencer content helps you:\n",
    "- Understand what visual elements are prominent in influencer posts\n",
    "- Extract semantic descriptions of brand visibility and product placement\n",
    "- Analyze whether images convey intended messaging\n",
    "- Process large volumes of influencer content programmatically\n",
    "- Create alternative text descriptions for accessibility\n",
    "\n",
    "## Workflow\n",
    "\n",
    "This notebook will:\n",
    "1. Load the BLIP model\n",
    "2. Find all images in a local folder (`./Images/`)\n",
    "3. Generate captions for each image\n",
    "4. Save results to a CSV file for analysis\n",
    "5. Display sample results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7a44b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Setup\n",
    "\n",
    "Import the necessary libraries for image processing, model loading, and data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad28f00",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set the paths and parameters for the captioning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration Settings\n",
    "\"\"\"\n",
    "\n",
    "# Path to folder containing images\n",
    "IMAGE_FOLDER = './Images'\n",
    "\n",
    "# File extensions to look for\n",
    "IMAGE_EXTENSIONS = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.webp']\n",
    "\n",
    "# Output file to save results\n",
    "OUTPUT_CSV = 'image_captions.csv'\n",
    "\n",
    "# Type of captioning\n",
    "# Options: \"unconditional\" or \"conditional\"\n",
    "CAPTIONING_MODE = \"unconditional\"\n",
    "\n",
    "# If using conditional captioning, provide a prompt\n",
    "# Example: \"a photo of\" or \"the main subject in this image is\"\n",
    "CONDITIONAL_PROMPT = \"a photo of\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e58818",
   "metadata": {},
   "source": [
    "## 3. Load the BLIP Model\n",
    "\n",
    "Download and load the pre-trained BLIP model. This may take a minute on first run as it downloads the model weights (~4GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BLIP model and processor...\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\", \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55dc18",
   "metadata": {},
   "source": [
    "## 4. Find All Images in the Folder\n",
    "\n",
    "Locate all image files in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all image files\n",
    "image_paths = []\n",
    "for extension in IMAGE_EXTENSIONS:\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_FOLDER, extension)))\n",
    "    # Also search in subdirectories\n",
    "    image_paths.extend(glob.glob(os.path.join(IMAGE_FOLDER, '**', extension), recursive=True))\n",
    "\n",
    "# Remove duplicates\n",
    "image_paths = list(set(image_paths))\n",
    "image_paths.sort()\n",
    "\n",
    "print(f\"Found {len(image_paths)} images in {IMAGE_FOLDER}\")\n",
    "if len(image_paths) > 0:\n",
    "    print(\"\\nFirst few images:\")\n",
    "    for img_path in image_paths[:5]:\n",
    "        print(f\"  - {img_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b95370",
   "metadata": {},
   "source": [
    "## 5. Generate Captions for All Images\n",
    "\n",
    "Process each image through the model and generate captions. This may take several minutes depending on the number of images and your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"\\nGenerating captions for {len(image_paths)} images...\")\n",
    "print(f\"Captioning mode: {CAPTIONING_MODE}\")\n",
    "\n",
    "for image_path in tqdm(image_paths):\n",
    "    try:\n",
    "        # Open and convert image\n",
    "        raw_image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Generate caption\n",
    "        if CAPTIONING_MODE == \"conditional\":\n",
    "            # Conditional captioning with prompt\n",
    "            inputs = processor(raw_image, CONDITIONAL_PROMPT, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        else:\n",
    "            # Unconditional captioning\n",
    "            inputs = processor(raw_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        \n",
    "        # Generate caption\n",
    "        out = model.generate(**inputs, max_length=50)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'image_path': image_path,\n",
    "            'caption': caption\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {image_path}: {str(e)}\")\n",
    "        results.append({\n",
    "            'image_path': image_path,\n",
    "            'caption': f\"Error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "print(f\"\\nSuccessfully generated captions for {len(results)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cb016",
   "metadata": {},
   "source": [
    "## 6. Save Results to CSV\n",
    "\n",
    "Save all captions to a CSV file for further analysis and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37975a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nDataFrame shape: {df_results.shape}\")\n",
    "print(\"\\nFirst 5 results:\")\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533adc35",
   "metadata": {},
   "source": [
    "## 7. Display Sample Results\n",
    "\n",
    "View captions alongside their images to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display, HTML\n",
    "\n",
    "# Display first 5 images with their captions\n",
    "num_samples = min(5, len(df_results))\n",
    "\n",
    "print(f\"Displaying first {num_samples} results:\\n\")\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    row = df_results.iloc[idx]\n",
    "    image_path = row['image_path']\n",
    "    caption = row['caption']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Image {idx+1}: {os.path.basename(image_path)}\")\n",
    "    print(f\"Caption: {caption}\")\n",
    "    print(f\"Full path: {image_path}\")\n",
    "    \n",
    "    # Display image if running in Jupyter\n",
    "    try:\n",
    "        display(IPImage(filename=image_path, width=400))\n",
    "    except:\n",
    "        print(\"(Image preview not available in terminal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942e028",
   "metadata": {},
   "source": [
    "## 8. (Optional) Try Different Prompts\n",
    "\n",
    "You can re-run the captioning with different prompts to guide the model. Change the settings above and re-run the captioning cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example prompts you might try:\n",
    "\n",
    "# For product-focused analysis\n",
    "- \"the main product shown in this image is\"\n",
    "- \"the brand elements visible in this image include\"\n",
    "\n",
    "# For influencer analysis\n",
    "- \"the person in this image is\"\n",
    "- \"the influencer appears to be\"\n",
    "\n",
    "# For general analysis\n",
    "- \"this image shows\"\n",
    "- \"a photo of\"\n",
    "\n",
    "# For engagement analysis\n",
    "- \"the emotional tone of this image is\"\n",
    "- \"this image conveys\"\n",
    "\n",
    "\n",
    "To use a different prompt:\n",
    "1. Change CAPTIONING_MODE to \"conditional\"\n",
    "2. Update CONDITIONAL_PROMPT with your desired prompt\n",
    "3. Update OUTPUT_CSV to a new filename\n",
    "4. Re-run the configuration and captioning cells\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ready to process images!\")\n",
    "print(f\"Images found: {len(image_paths)}\")\n",
    "print(f\"Current mode: {CAPTIONING_MODE}\")\n",
    "if CAPTIONING_MODE == \"conditional\":\n",
    "    print(f\"Current prompt: '{CONDITIONAL_PROMPT}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
