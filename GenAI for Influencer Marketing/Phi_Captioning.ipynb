{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0cc060a6",
      "metadata": {
        "id": "0cc060a6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marketing-and-customer-insight/AI_For_Marketing/blob/main/GenAI%20for%20Influencer%20Marketing/Phi_Captioning.ipynb)\n",
        "\n",
        "In Google Colab please make sure to select: Runtime -> Change Runtime -> Tesla T4 (GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a1108d",
      "metadata": {
        "id": "61a1108d"
      },
      "source": [
        "# Phi-4 Multimodal Image Captioning for Influencer Content Analysis\n",
        "\n",
        "This notebook demonstrates how to use Phi-4-multimodal-instruct, a state-of-the-art Vision Language Model (VLM) from Microsoft, to automatically generate captions for images in a local folder.\n",
        "\n",
        "## What is Phi-4 Multimodal?\n",
        "\n",
        "Phi-4 is Microsoft's advanced multimodal AI model that seamlessly combines vision and language understanding. It can process images and generate detailed, contextually-aware text descriptions. The model excels at:\n",
        "\n",
        "1. **Detailed Image Understanding**: Analyzing visual content with nuanced descriptions\n",
        "2. **Instruction-Based Captioning**: Responding to specific prompts about images with tailored output\n",
        "3. **Rich Context Generation**: Creating comprehensive descriptions that capture objects, styles, and semantic meaning\n",
        "\n",
        "## Use Case for Influencer Marketing\n",
        "\n",
        "Automatically generating captions for influencer content with Phi-4 helps you:\n",
        "- Extract precise descriptions of visual elements in influencer posts\n",
        "- Understand brand visibility and product placement in images\n",
        "- Analyze whether images convey intended messaging with detailed context\n",
        "- Process large volumes of influencer content programmatically\n",
        "- Create comprehensive alternative text descriptions for accessibility\n",
        "- Generate marketing insights from visual content analysis\n",
        "\n",
        "## Workflow\n",
        "\n",
        "This notebook will:\n",
        "1. Load the Phi-4 multimodal model\n",
        "2. Find all images in a local folder (`./Images/`)\n",
        "3. Generate detailed captions for each image using Phi-4\n",
        "4. Save results to a CSV file for analysis\n",
        "5. Display sample results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca7a44b",
      "metadata": {
        "id": "8ca7a44b"
      },
      "source": [
        "## 1. Import Required Libraries and Setup\n",
        "\n",
        "Import the necessary libraries for image processing, model loading, and data management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8acf967b",
      "metadata": {
        "id": "8acf967b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.48.2 accelerate evaluate backoff --quiet\n",
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b71e47",
      "metadata": {
        "id": "f1b71e47"
      },
      "source": [
        "Download existing images from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ef8ba7",
      "metadata": {
        "id": "47ef8ba7"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 --filter=blob:none --sparse \\\n",
        "https://github.com/marketing-and-customer-insight/AI_For_Marketing.git\n",
        "%cd AI_For_Marketing\n",
        "!git sparse-checkout set \"GenAI for Influencer Marketing/Images\"\n",
        "!mv \"GenAI for Influencer Marketing/Images\" /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad28f00",
      "metadata": {
        "id": "fad28f00"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set the paths and parameters for the captioning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd4f82b",
      "metadata": {
        "id": "cbd4f82b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration Settings\n",
        "\"\"\"\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# Path to folder containing images\n",
        "IMAGE_FOLDER = './Images'\n",
        "\n",
        "# File extensions to look for\n",
        "IMAGE_EXTENSIONS = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.webp']\n",
        "\n",
        "# Find all image files\n",
        "image_paths = []\n",
        "for extension in IMAGE_EXTENSIONS:\n",
        "    image_paths.extend(glob.glob(os.path.join(IMAGE_FOLDER, extension)))\n",
        "    # Also search in subdirectories\n",
        "    image_paths.extend(glob.glob(os.path.join(IMAGE_FOLDER, '**', extension), recursive=True))\n",
        "\n",
        "# Remove duplicates\n",
        "image_paths = list(set(image_paths))\n",
        "image_paths.sort()\n",
        "\n",
        "print(f\"Found {len(image_paths)} images in {IMAGE_FOLDER}\")\n",
        "if len(image_paths) > 0:\n",
        "    for img_path in image_paths:\n",
        "        print(f\"  - {img_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e58818",
      "metadata": {
        "id": "b9e58818"
      },
      "source": [
        "## 3. Load the Phi-4 Multimodal Vision Language Model\n",
        "\n",
        "Download and load the pre-trained Phi-4 multimodal model. This may take a minute on first run as it downloads the model weights (~4GB). The model will be loaded with float16 precision for optimal GPU memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2669dc6b",
      "metadata": {
        "id": "2669dc6b"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    _attn_implementation='eager'\n",
        ")\n",
        "model.eval()\n",
        "model.requires_grad_(False)\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(model_path)\n",
        "\n",
        "def predict_phi(inference_img_path:str, prompt:str, max_tokens:int=128):\n",
        "    image_predict = Image.open(inference_img_path).convert('RGB').resize((224, 224))\n",
        "\n",
        "    inputs = processor(text=prompt, images=image_predict, return_tensors='pt').to(device)\n",
        "    with torch.inference_mode():\n",
        "        generate_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    response = processor.batch_decode(\n",
        "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b95370",
      "metadata": {
        "id": "f7b95370"
      },
      "source": [
        "## 5. Generate Captions for All Images\n",
        "\n",
        "Process each image through the model and generate captions. This may take several minutes depending on the number of images and your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6UNdda0LrfP5",
      "metadata": {
        "id": "6UNdda0LrfP5"
      },
      "outputs": [],
      "source": [
        "# Output file to save results\n",
        "OUTPUT_CSV = 'image_captions.csv'\n",
        "\n",
        "PROMPT = \"Describe all objects and visual style of the image in detail.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef4b7f5",
      "metadata": {
        "id": "aef4b7f5"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "print(f\"\\nGenerating captions for {len(image_paths)} images...\")\n",
        "\n",
        "for image_path in tqdm(image_paths):\n",
        "    try:\n",
        "        caption = predict_phi(image_path, PROMPT, max_tokens=128)\n",
        "\n",
        "        # Store result\n",
        "        results.append({\n",
        "            'image_path': image_path,\n",
        "            'caption': caption\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing {image_path}: {str(e)}\")\n",
        "        results.append({\n",
        "            'image_path': image_path,\n",
        "            'caption': f\"Error: {str(e)}\"\n",
        "        })\n",
        "\n",
        "print(f\"\\nSuccessfully generated captions for {len(results)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7cb016",
      "metadata": {
        "id": "0e7cb016"
      },
      "source": [
        "## 6. Save Results to CSV\n",
        "\n",
        "Save all captions to a CSV file for further analysis and review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d37975a0",
      "metadata": {
        "id": "d37975a0"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "df_results.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Results saved to: {OUTPUT_CSV}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\nFirst 5 results:\")\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533adc35",
      "metadata": {
        "id": "533adc35"
      },
      "source": [
        "## 7. Display Sample Results\n",
        "\n",
        "View captions alongside their images to verify quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55dc8e52",
      "metadata": {
        "id": "55dc8e52"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as IPImage, display, HTML\n",
        "\n",
        "# Display first 5 images with their captions\n",
        "num_samples = min(5, len(df_results))\n",
        "\n",
        "print(f\"Displaying first {num_samples} results:\\n\")\n",
        "\n",
        "for idx in range(num_samples):\n",
        "    row = df_results.iloc[idx]\n",
        "    image_path = row['image_path']\n",
        "    caption = row['caption']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Image {idx+1}: {os.path.basename(image_path)}\")\n",
        "    print(f\"Caption: {caption}\")\n",
        "    print(f\"Full path: {image_path}\")\n",
        "\n",
        "    # Display image if running in Jupyter\n",
        "    try:\n",
        "        display(IPImage(filename=image_path, width=400))\n",
        "    except:\n",
        "        print(\"(Image preview not available in terminal)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
